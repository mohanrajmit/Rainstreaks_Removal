{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled20.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMJsVVXuFFnlY4U2FnlaXMs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohanrajmit/Rainstreaks_Removal/blob/main/model_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "iAayH1kSLiNH",
        "outputId": "6a315164-8410-494e-af35-dd4bf1f3f5a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.19.5\n",
            "  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.8 MB 4.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.5\n",
            "    Uninstalling numpy-1.21.5:\n",
            "      Successfully uninstalled numpy-1.21.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "tensorflow 2.8.0 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed numpy-1.19.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install numpy==1.19.5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/mohanrajmit/rainstreaks_dataset.git"
      ],
      "metadata": {
        "id": "0dQHTNIdMsxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 1.x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3F4F1yLXLnVc",
        "outputId": "ae57b887-87d9-48fa-820f-7ff5b0f0abdd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from GuidedFilter import guided_filter\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "ZZWGZspQLjGL"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "##################### Select GPU device ####################################\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
        "############################################################################\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "##################### Network parameters ###################################\n",
        "num_feature = 16             # number of feature maps\n",
        "num_channels = 3             # number of input's channels \n",
        "patch_size = 64              # patch size \n",
        "KernelSize = 3               # kernel size \n",
        "learning_rate = 0.1          # learning rate\n",
        "iterations = int(2.1*1e5)    # iterations\n",
        "batch_size = 20              # batch size\n",
        "save_model_path = \"./model/\" # saved model's path\n",
        "model_name = 'model-epoch'   # saved model's name\n",
        "############################################################################\n",
        "\n",
        "\n",
        "input_path = \"./TrainData/input/\"    # the path of rainy images\n",
        "gt_path = \"./TrainData/label/\"       # the path of ground truth\n",
        "\n",
        "\n",
        "input_files = os.listdir(input_path)\n",
        "gt_files = os.listdir(gt_path) \n"
      ],
      "metadata": {
        "id": "czjP3-98L9IM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " \n",
        "# randomly select image patches\n",
        "def _parse_function(filename, label):  \n",
        "     \n",
        "  image_string = tf.read_file(filename)  \n",
        "  image_decoded = tf.image.decode_jpeg(image_string, channels=3)  \n",
        "  rainy = tf.cast(image_decoded, tf.float32)/255.0\n",
        "  \n",
        "  \n",
        "  image_string = tf.read_file(label)  \n",
        "  image_decoded = tf.image.decode_jpeg(image_string, channels=3)  \n",
        "  label = tf.cast(image_decoded, tf.float32)/255.0\n",
        "\n",
        "  t = time.time()\n",
        "  rainy = tf.random_crop(rainy, [patch_size, patch_size ,3],seed = t)   # randomly select patch\n",
        "  label = tf.random_crop(label, [patch_size, patch_size ,3],seed = t)   \n",
        "  return rainy, label \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "u8CB_52DMG90"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# network structure\n",
        "def inference(images, is_training):\n",
        "    regularizer = tf.contrib.layers.l2_regularizer(scale = 1e-10)\n",
        "    initializer = tf.contrib.layers.xavier_initializer()\n",
        "\n",
        "    base = guided_filter(images, images, 15, 1, nhwc=True) # using guided filter for obtaining base layer\n",
        "    detail = images - base   # detail layer\n",
        "\n",
        "   #  layer 1\n",
        "    with tf.variable_scope('layer_1'):\n",
        "         output = tf.layers.conv2d(detail, num_feature, KernelSize, padding = 'same', kernel_initializer = initializer, \n",
        "                                   kernel_regularizer = regularizer, name='conv_1')\n",
        "         output = tf.layers.batch_normalization(output, training=is_training, name='bn_1')\n",
        "         output_shortcut = tf.nn.relu(output, name='relu_1')\n",
        "  \n",
        "   #  layers 2 to 25\n",
        "    for i in range(12):\n",
        "        with tf.variable_scope('layer_%d'%(i*2+2)):\t\n",
        "             output = tf.layers.conv2d(output_shortcut, num_feature, KernelSize, padding='same', kernel_initializer = initializer, \n",
        "                                       kernel_regularizer = regularizer, name=('conv_%d'%(i*2+2)))\n",
        "             output = tf.layers.batch_normalization(output, training=is_training, name=('bn_%d'%(i*2+2)))\t\n",
        "             output = tf.nn.relu(output, name=('relu_%d'%(i*2+2)))\n",
        "\n",
        "\n",
        "        with tf.variable_scope('layer_%d'%(i*2+3)): \n",
        "             output = tf.layers.conv2d(output, num_feature, KernelSize, padding='same', kernel_initializer = initializer,\n",
        "                                       kernel_regularizer = regularizer, name=('conv_%d'%(i*2+3)))\n",
        "             output = tf.layers.batch_normalization(output, training=is_training, name=('bn_%d'%(i*2+3)))\n",
        "             output = tf.nn.relu(output, name=('relu_%d'%(i*2+3)))\n",
        "\n",
        "        output_shortcut = tf.add(output_shortcut, output)   # shortcut\n",
        "\n",
        "   # layer 26\n",
        "    with tf.variable_scope('layer_26'):\n",
        "         output = tf.layers.conv2d(output_shortcut, num_channels, KernelSize, padding='same',   kernel_initializer = initializer, \n",
        "                                   kernel_regularizer = regularizer, name='conv_26')\n",
        "         neg_residual = tf.layers.batch_normalization(output, training=is_training, name='bn_26')\n",
        "\n",
        "    final_out = tf.add(images, neg_residual)\n",
        "\n",
        "    return final_out\n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "id": "cL1Kp7zCMM9j"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':   \n",
        "   RainName = os.listdir(input_path)\n",
        "   for i in range(len(RainName)):\n",
        "      RainName[i] = input_path + RainName[i]\n",
        "      \n",
        "   LabelName = os.listdir(gt_path)    \n",
        "   for i in range(len(LabelName)):\n",
        "       LabelName[i] = gt_path + LabelName[i] \n",
        "    \n",
        "   filename_tensor = tf.convert_to_tensor(RainName, dtype=tf.string)  \n",
        "   labels_tensor = tf.convert_to_tensor(LabelName, dtype=tf.string)   \n",
        "   dataset = tf.data.Dataset.from_tensor_slices((filename_tensor, labels_tensor))\n",
        "   dataset = dataset.map(_parse_function)    \n",
        "   dataset = dataset.prefetch(buffer_size=batch_size * 10)\n",
        "   dataset = dataset.batch(batch_size).repeat()  \n",
        "   iterator = dataset.make_one_shot_iterator()\n",
        "   \n",
        "   rainy, labels = iterator.get_next()     \n",
        "   \n",
        "   \n",
        "   outputs = inference(rainy, is_training = True)\n",
        "   loss = tf.reduce_mean(tf.square(labels - outputs))    # MSE loss\n",
        "\n",
        "   \n",
        "   lr_ = learning_rate\n",
        "   lr = tf.placeholder(tf.float32 ,shape = [])  \n",
        "\n",
        "   update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "   with tf.control_dependencies(update_ops):\n",
        "        train_op =  tf.train.MomentumOptimizer(lr, 0.9).minimize(loss) \n",
        "\n",
        "   \n",
        "   all_vars = tf.trainable_variables()   \n",
        "   g_list = tf.global_variables()\n",
        "   bn_moving_vars = [g for g in g_list if 'moving_mean' in g.name]\n",
        "   bn_moving_vars += [g for g in g_list if 'moving_variance' in g.name]\n",
        "   all_vars += bn_moving_vars\n",
        "   print(\"Total parameters' number: %d\" %(np.sum([np.prod(v.get_shape().as_list()) for v in all_vars])))  \n",
        "   saver = tf.train.Saver(var_list=all_vars, max_to_keep=5)\n",
        "   \n",
        "   \n",
        "   config = tf.ConfigProto()\n",
        "   config.gpu_options.per_process_gpu_memory_fraction = 0.8 # GPU setting\n",
        "   config.gpu_options.allow_growth = True\n",
        "   init =  tf.group(tf.global_variables_initializer(), \n",
        "                         tf.local_variables_initializer())  \n",
        "    \n",
        "   with tf.Session(config=config) as sess:      \n",
        "      with tf.device('/gpu:0'): \n",
        "            sess.run(init)\n",
        "            tf.get_default_graph().finalize()\n",
        "            \n",
        "            if tf.train.get_checkpoint_state('./model/'):   # load previous trained models\n",
        "               ckpt = tf.train.latest_checkpoint('./model/')\n",
        "               saver.restore(sess, ckpt)\n",
        "               ckpt_num = re.findall(r'(\\w*[0-9]+)\\w*',ckpt)\n",
        "               start_point = int(ckpt_num[0]) + 1   \n",
        "               print(\"successfully load previous model\")\n",
        "       \n",
        "            else:   # re-training if no previous trained models\n",
        "               start_point = 0    \n",
        "               print(\"re-training\")\n",
        "    \n",
        "    \n",
        "            check_data, check_label = sess.run([rainy, labels])\n",
        "            print(\"Check patch pair:\")  \n",
        "            plt.subplot(1,2,1)     \n",
        "            plt.imshow(check_data[0,:,:,:])\n",
        "            plt.title('input')         \n",
        "            plt.subplot(1,2,2)    \n",
        "            plt.imshow(check_label[0,:,:,:])\n",
        "            plt.title('ground truth')        \n",
        "            plt.show()\n",
        "    \n",
        "    \n",
        "            start = time.time()  \n",
        "            \n",
        "            for j in range(start_point,iterations):   #  iterations\n",
        "                if j+1 > int(1e5):\n",
        "                    lr_ = learning_rate*0.1\n",
        "                if j+1 > int(2e5):\n",
        "                    lr_ = learning_rate*0.01             \n",
        "                    \n",
        "    \n",
        "                _,Training_Loss = sess.run([train_op,loss], feed_dict={lr: lr_}) # training\n",
        "          \n",
        "                if np.mod(j+1,100) == 0 and j != 0: # save the model every 100 iterations\n",
        "                   end = time.time()              \n",
        "                   print ('%d / %d iteraions, learning rate = %.3f, Training Loss = %.4f, runtime = %.1f s' \n",
        "                          % (j+1, iterations, lr_, Training_Loss, (end - start)))                  \n",
        "                   save_path_full = os.path.join(save_model_path, model_name) # save model\n",
        "                   saver.save(sess, save_path_full, global_step = j+1, write_meta_graph=False)\n",
        "                   start = time.time()\n",
        "                   \n",
        "            print('Training is finished.')\n",
        "   sess.close() "
      ],
      "metadata": {
        "id": "jTVNsdNZMTZD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}